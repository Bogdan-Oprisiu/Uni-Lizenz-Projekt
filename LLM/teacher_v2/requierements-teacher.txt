# Teacher Pipeline â€“ Quickâ€‘Start Guide

This short guide explains **what you need** and **how to run** the T5â€‘based teacher pipeline that generates softâ€‘targets for the student model.

---
## 1Â·Folder layout
```
LLM/                                â† project root (call this $LLM_ROOT)
â”‚
â”œâ”€ teacher_v2/                      â† this repo â€“ fineâ€‘tuning + softâ€‘target code
â”‚   â”œâ”€ fine_tune_teacher.py         â† supervised fineâ€‘tuning script
â”‚   â”œâ”€ extract_soft_targets.py      â† runs teacher over unlabeled corpus
â”‚   â”œâ”€ config.py                    â† central paths/hyperâ€‘params
â”‚   â”œâ”€ utils.py                     â† helpers (tokenizer/model I/O, batchingâ€¦)
â”‚   â””â”€ â€¦
â”‚
â”œâ”€ tokenizer_v2/                    â† shared BPE tokenizer
â”‚   â”œâ”€ bpe_tokenizer_v2.json        â† tokenizer model (â‰ˆ60k merges)
â”‚   â””â”€ utils.py (load_tokenizer helper)
â”‚
â””â”€ training_data/                   â† corpora (labeled + unlabeled)
    â”œâ”€ basic_data/
    â””â”€ multiple_parameter_data/
```

---
## 2Â·Python requirements
```bash
pip install --upgrade pip
# 2.1 core libraries
pip install torch==2.0.1+cu117 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117
pip install transformers==4.34.0   # works with torch 2.0 & no torch.compiler issues
pip install datasets sentencepiece tokenizers tqdm huggingface-hub accelerate

# 2.2 optional (for 8â€‘bit loading)
# pip install bitsandbytes

# 2.3 preprocessing helpers (if you use the spelling/units pipeline)
pip install pyspellchecker
```
> **Colab hint** â€“ if you store the repo in `/content/Uni-Lizenz-Projekt/LLM` add:
> ```python
> import sys
> sys.path.insert(0, "/content/Uni-Lizenz-Projekt/LLM")  # teacher_v2 & tokenizer_v2 imports
> ```

---
## 3Â·Fineâ€‘tune the teacher (optional)
If you havenâ€™t already trained a checkpoint, run
```bash
python teacher_v2/fine_tune_teacher.py \
    --variant basic        # or multi
    --epochs 3             # tweak as needed
    --batch_size 8         # fit to your GPUâ€‘VRAM
    --lr 2e-4              # learningâ€‘rate
    --output_dir teacher_v2/finetuned_teacher
```
This will save both **model** and **tokenizer** to `teacher_v2/finetuned_teacher/`.
`config.py` automatically prefers that directory when it exists.

---
## 4Â·Extract soft targets
```bash
python teacher_v2/extract_soft_targets.py \
    --batch_size 8 \
    --top_k 5 \
    --temperature 2.0 \
    --compress          # writes soft_targets_top5.json.gz
```
Outputs land in:
* `artefacts/teacher_outputs/soft_targets_top5.json(.gz)`  â€“ listâ€‘ofâ€‘lists of (token_id, prob)
* `artefacts/teacher_outputs/teacher_predictions.jsonl`    â€“ one decoded JSON command per line

---
## 5Â·Common gotchas
| Symptom | Fix |
|---------|-----|
| **`ModuleNotFoundError: tokenizer_v2`** | Ensure `$LLM_ROOT` is on `sys.path` **and** `tokenizer_v2/__init__.py` exists (`touch` an empty file). |
| **`module 'torch' has no attribute 'compiler'`** | Use *torchâ‰¥2.0* **or** downgrade transformers to `4.34.0` or below. |
| **`model_kwargs not used: ['token_type_ids']`** | The shipped `extract_soft_targets.py` already drops that key â€“ make sure you copied the latest file. |
| OOM on T5â€‘Large | Try `DEVICE="cpu"` in `config.py`, smaller `--batch_size`, or enable 8â€‘bit with bitsandbytes. |

---
## 6Â·Troubleshooting checklist
1. `python -c "import torch, transformers, huggingface_hub, accelerate; print(torch.__version__, transformers.__version__)"`
2. `print(config.FINETUNED_CKPT, config.TOKENIZER_JSON)` â€“ check paths exist.
3. Verify `training_data/*` corpora are present (otherwise `assert_paths()` fails).

Once those three points pass, the teacher pipeline should run endâ€‘toâ€‘end.

Happy trainingðŸš‚ðŸŽ‰

