# tokenizer_v2 – dependency pinning
# ----------------------------------
#
# This `requirements-tokenizer.txt` file is **only** for the standalone
# tokenizer‑training pipeline found in `tokenizer_v2/`.
# You can install it into an activated virtual‑env via:
#
#     pip install -r requirements-tokenizer.txt
#
# ─── Folder / file layout expected by `tokenizer_v2/train.py` ───────────────
#
# .
# ├─ training_data/
# │  ├─ basic_data/
# │  │ ├─ synthetic_basic_labeled_robot_commands_json.txt
# │  │ └─ synthetic_basic_unlabeled_robot_commands.txt
# │  ├─ multiple_parameter_data/
# │  │ ├─ synthetic_labeled_robot_commands_with_accel_json.txt
# │  │ └─ synthetic_unlabeled_robot_commands_with_accel.txt
# │  └─ tokenizer/
# │     ├─ google-10000-english-no-swears.txt   # replicated dictionary
# │     └─ units_and_numbers.txt                # generated by generate_units_and_numbers_file.py
# └─ tokenizer_v2/
#     ├─ __init__.py
#     ├─ config.py
#     ├─ utils.py
#     ├─ train.py
#     └─ custom_normalizer.py     # optional– your own normaliser factory
#
#   *If `custom_normalizer.py` is absent, the default lowercase+ASCII normaliser
#   described in the thesis is used.*
#
# ─── Output location ────────────────────────────────────────────────────────
# After a successful run you will find three artefacts under
#
#     artefacts/tokenizer_v2/
#          ├─ bpe_tokenizer_v2.json              # full vocabulary + merges
#          ├─ bpe_tokenizer_v2_config.json       # HF‑compatible special tokens
#          └─ bpe_tokenizer_v2_training_params.json
#
# The `config.py` constant `OUTPUT_DIR` controls this path if you need to move
# it elsewhere.
#
# ─── Dependencies ───────────────────────────────────────────────────────────
# Only lightweight packages are required.  Versions are pinned loosely to avoid
# conflicts with the rest of your project; tighten if you need
# deterministic builds.

# Core tokeniser engine
# (0.15.x is latest stable as of 2025‑04; older versions work but lack some
#  BPE options used in train.py)
tokenizers>=0.15

# Optional progress bar – train.py will skip gracefully if missing
tqdm>=4.66

# Needed for regex replacement in custom normaliser (already bundled with CPython)
regex>=2023.0   # only if your custom_normalizer.py imports it

